{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9207713b",
   "metadata": {},
   "source": [
    "# Assignment Submission - Sarvam Research Fellowship \n",
    "\n",
    "I plan to obtain results for these two combinations\n",
    "\n",
    "1. Pre-trained Fast-text vectors - Supervised Procrustes method\n",
    "2. Pre-trained Fast-text vectors - Unsupervised method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad261905",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import faiss\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8fe9a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc62c788",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806c2ee2",
   "metadata": {},
   "source": [
    "## 1. Data Preparation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3dc6a2",
   "metadata": {},
   "source": [
    "\n",
    "### 1.1 Pre-trained Fast-text vectors and limit vocab to top 100K most frequent words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80caa91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(file_path, max_words=100000):\n",
    "    model = fasttext.load_model(file_path)\n",
    "    words = model.get_words()[:max_words] \n",
    "    embeddings = {}\n",
    "    \n",
    "    for word in words:\n",
    "        vector = model.get_word_vector(word)\n",
    "        vector = vector / np.linalg.norm(vector) \n",
    "        embeddings[word] = vector\n",
    "    \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "424480d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 100000 English and 100000 Hindi embeddings.\n"
     ]
    }
   ],
   "source": [
    "en_embeddings = load_embeddings('./Fasttext_Embeddings/cc.en.300.bin')\n",
    "hi_embeddings = load_embeddings('./Fasttext_Embeddings/cc.hi.300.bin')\n",
    "print(f\"Loaded {len(en_embeddings)} English and {len(hi_embeddings)} Hindi embeddings.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0a6b25",
   "metadata": {},
   "source": [
    "###  1.3 Extract a list of word translation pairs from the MUSE dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7f30390",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_MUSE_dictionary(file_path, src_emb, tgt_emb):\n",
    "    pairs = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            src_word, tgt_word = line.strip().split()\n",
    "            if src_word in src_emb and tgt_word in tgt_emb:\n",
    "                pairs.append((src_word, tgt_word))\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66ffeab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training pairs: 8130, Test pairs: 1600\n"
     ]
    }
   ],
   "source": [
    "train_pairs = load_MUSE_dictionary('./MUSE/en-hi.train.txt', en_embeddings, hi_embeddings)\n",
    "test_pairs = load_MUSE_dictionary('./MUSE/en-hi.test.txt', en_embeddings, hi_embeddings)\n",
    "print(f\"Training pairs: {len(train_pairs)}, Test pairs: {len(test_pairs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0283027a",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Embedding Alignment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1194cf",
   "metadata": {},
   "source": [
    "### 2.1 Implementing the Procrustes Alignment Method and ensure that the mapping is orthogonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1ef1816",
   "metadata": {},
   "outputs": [],
   "source": [
    "def procrustes_alignment(train_pairs, src_emb, tgt_emb):\n",
    "    # Build matrices X (source) and Y (target)\n",
    "    X = np.array([src_emb[src_word] for src_word, _ in train_pairs]).T  # Shape: (300, n_pairs)\n",
    "    Y = np.array([tgt_emb[tgt_word] for _, tgt_word in train_pairs]).T  # Shape: (300, n_pairs)\n",
    "    \n",
    "    # Compute the mapping\n",
    "    M = Y @ X.T  # Shape: (300, 300)\n",
    "    U, _, Vh = np.linalg.svd(M)\n",
    "    W = U @ Vh  # Orthogonal matrix: (300, 300)\n",
    "    \n",
    "    # Verify orthogonality (W @ W.T should be close to identity)\n",
    "    assert np.allclose(W @ W.T, np.eye(300), atol=1e-6), \"W is not orthogonal\"\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37eec44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = procrustes_alignment(train_pairs, en_embeddings, hi_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551e2926",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1228349c",
   "metadata": {},
   "source": [
    "### 3.1 Perform word translation from English to Hindi using the aligned embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68689d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is slow - so I used the faiss library to improve the speed\n",
    "# def translate_word(src_word, W, src_emb, tgt_emb):\n",
    "#     if src_word not in src_emb:\n",
    "#         return []\n",
    "#     src_vec = W @ src_emb[src_word]  # Map to target space\n",
    "#     similarities = [(tgt_word, np.dot(src_vec, tgt_vec)) \n",
    "#                     for tgt_word, tgt_vec in tgt_emb.items()]\n",
    "#     similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "#     return [tgt_word for tgt_word, _ in similarities[:5]]\n",
    "# translations = {src: translate_word(src, W, en_embeddings, hi_embeddings) \n",
    "#                 for src, _ in test_pairs}\n",
    "# print(\"Sample translations:\", translations[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a6025e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_index(embeddings):\n",
    "    dim = 300\n",
    "    index = faiss.IndexFlatIP(dim) \n",
    "    vectors = np.array(list(embeddings.values())).astype('float32')\n",
    "    index.add(vectors)\n",
    "    return index, list(embeddings.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cce1eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build index for Hindi embeddings\n",
    "hi_index, hi_words = build_index(hi_embeddings)\n",
    "\n",
    "def translate_word_faiss(src_word, W, src_emb, index, tgt_words):\n",
    "    if src_word not in src_emb:\n",
    "        return []\n",
    "    src_vec = (W @ src_emb[src_word]).reshape(1, -1).astype('float32')\n",
    "    distances, indices = index.search(src_vec, 5)\n",
    "    return [tgt_words[i] for i in indices[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c397f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "translations = {src: translate_word_faiss(src, W, en_embeddings, hi_index, hi_words) for src, _ in test_pairs}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b28590c",
   "metadata": {},
   "source": [
    "### 3.2 Evaluate translation accuracy using the MUSE test dictionary  and report Precision@1 and Precision5 metrics for the word transaltion task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d299e1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_translation(test_pairs, translations):\n",
    "    p1, p5 = 0, 0\n",
    "    for src, correct_tgts in test_pairs:\n",
    "        if src not in translations:\n",
    "            continue\n",
    "        predicted = translations[src]\n",
    "        correct_tgts = set(correct_tgts.split()) if isinstance(correct_tgts, str) else {correct_tgts}\n",
    "        p1 += 1 if predicted[0] in correct_tgts else 0\n",
    "        p5 += 1 if any(tgt in correct_tgts for tgt in predicted) else 0\n",
    "    total = len([src for src, _ in test_pairs if src in translations])\n",
    "    return p1 / total * 100, p5 / total * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87aa24a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision@1: 33.12%, Precision@5: 53.12%\n"
     ]
    }
   ],
   "source": [
    "# Compute metrics\n",
    "p1, p5 = evaluate_translation(test_pairs, translations)\n",
    "print(f\"Precision@1: {p1:.2f}%, Precision@5: {p5:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50495971",
   "metadata": {},
   "source": [
    "### 3.3 Compare and Analyse cosine similarites between word pairs to assess cross-lingual semantic similarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e65bc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_similarity(src_word, tgt_word, W, src_emb, tgt_emb):\n",
    "    if src_word not in src_emb or tgt_word not in tgt_emb:\n",
    "        return None\n",
    "    mapped_vec = W @ src_emb[src_word]\n",
    "    return np.dot(mapped_vec, tgt_emb[tgt_word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "248a9e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg similarity (true pairs): 0.4135, Avg similarity (random pairs): 0.0883\n"
     ]
    }
   ],
   "source": [
    "# True pairs\n",
    "true_sims = [compute_similarity(src, tgt, W, en_embeddings, hi_embeddings) for src, tgt in test_pairs if compute_similarity(src, tgt, W, en_embeddings, hi_embeddings) is not None]\n",
    "true_avg = np.mean(true_sims)\n",
    "\n",
    "# Random pairs\n",
    "np.random.shuffle(test_pairs)\n",
    "random_pairs = [(test_pairs[i][0], test_pairs[i+1][1]) for i in range(len(test_pairs)-1)]\n",
    "random_sims = [compute_similarity(src, tgt, W, en_embeddings, hi_embeddings) for src, tgt in random_pairs if compute_similarity(src, tgt, W, en_embeddings, hi_embeddings) is not None]\n",
    "random_avg = np.mean(random_sims)\n",
    "\n",
    "print(f\"Avg similarity (true pairs): {true_avg:.4f}, Avg similarity (random pairs): {random_avg:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d4e52e",
   "metadata": {},
   "source": [
    "### 3.4 Conduct an ablation study to assess the impact of bilingual lexicon size on alignment quality. Experiment with different training dictionary sizes (e.g., 5k, 10k, 20k word pairs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b6b386",
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = [1000, 2000, 5000]\n",
    "results = {}\n",
    "\n",
    "for size in tqdm(sizes):\n",
    "    sampled_pairs = np.random.choice(len(train_pairs), size, replace=False)\n",
    "    sampled_train = [train_pairs[i] for i in sampled_pairs]\n",
    "    W_sampled = procrustes_alignment(sampled_train, en_embeddings, hi_embeddings)\n",
    "    sampled_translations = {src: translate_word_faiss(src, W_sampled, en_embeddings, hi_index, hi_words) for src, _ in test_pairs}\n",
    "    p1, p5 = evaluate_translation(test_pairs, sampled_translations)\n",
    "    results[size] = (p1, p5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "239d4c5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size 1000: P@1 = 18.06%, P@5 = 37.44%\n",
      "Training size 2000: P@1 = 22.12%, P@5 = 47.44%\n",
      "Training size 5000: P@1 = 26.88%, P@5 = 52.25%\n"
     ]
    }
   ],
   "source": [
    "# Report results\n",
    "for size, (p1, p5) in results.items():\n",
    "    print(f\"Training size {size}: P@1 = {p1:.2f}%, P@5 = {p5:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2a78db",
   "metadata": {},
   "source": [
    "## 4. Unsupervised alignment method: Cross-Domain Similarity Local Scaling (CSLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4eac4530",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_csls_scores(src_vec, tgt_vecs, k=10):\n",
    "    # Convert to numpy for FAISS\n",
    "    src_vec = np.array(src_vec, dtype=np.float32).reshape(1, -1)\n",
    "    tgt_vecs = np.array(tgt_vecs, dtype=np.float32)\n",
    "    \n",
    "    # Build FAISS index for target vectors\n",
    "    index = faiss.IndexFlatL2(tgt_vecs.shape[1])\n",
    "    index.add(tgt_vecs)\n",
    "    \n",
    "    # Compute mean similarity to k nearest neighbors in target space\n",
    "    distances, _ = index.search(tgt_vecs, k)\n",
    "    r_T = np.mean(distances, axis=1)\n",
    "    \n",
    "    # Compute distances from source vector to all target vectors\n",
    "    distances, indices = index.search(src_vec, len(tgt_vecs))\n",
    "    distances = distances.flatten()\n",
    "    indices = indices.flatten()\n",
    "    \n",
    "    # CSLS score: 2 * cosine_similarity - r_S - r_T\n",
    "    src_norm = src_vec / np.linalg.norm(src_vec)\n",
    "    tgt_norm = tgt_vecs / np.linalg.norm(tgt_vecs, axis=1)[:, None]\n",
    "    cos_sim = np.dot(src_norm, tgt_norm.T).flatten()\n",
    "    r_S = np.mean(distances[:k])  # Mean distance to k nearest neighbors of src_vec\n",
    "    csls_scores = 2 * cos_sim - r_S - r_T\n",
    "    \n",
    "    return csls_scores, indices\n",
    "\n",
    "def translate_word_csls(src_word, W, src_emb, tgt_emb, tgt_words, k=5):\n",
    "    mapped_vec = W @ src_emb[src_word]\n",
    "    tgt_vecs = np.array([tgt_emb[word] for word in tgt_words])\n",
    "    csls_scores, indices = compute_csls_scores(mapped_vec, tgt_vecs)\n",
    "    top_k_indices = np.argsort(-csls_scores)[:k]\n",
    "    return [tgt_words[idx] for idx in top_k_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7130170e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim=300, hidden_dim=2048):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "def adversarial_training(src_emb, tgt_emb, epochs=50, batch_size=32, lr=0.001):\n",
    "    # Prepare data\n",
    "    src_words = list(src_emb.keys())[:50000]  # Use top 50,000 frequent words\n",
    "    tgt_words = list(tgt_emb.keys())[:50000]\n",
    "    src_vecs = torch.tensor([src_emb[w] for w in src_words], dtype=torch.float32).to(device)\n",
    "    tgt_vecs = torch.tensor([tgt_emb[w] for w in tgt_words], dtype=torch.float32).to(device)\n",
    "    \n",
    "    # Initialize W (orthogonal matrix)\n",
    "    W = torch.eye(300, dtype=torch.float32).to(device)\n",
    "    W.requires_grad = True\n",
    "    \n",
    "    # Models and optimizers\n",
    "    discriminator = Discriminator().to(device)\n",
    "    optimizer_W = optim.Adam([W], lr=lr)\n",
    "    optimizer_D = optim.Adam(discriminator.parameters(), lr=lr)\n",
    "    \n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        # Train discriminator (5 steps)\n",
    "        for _ in range(5):\n",
    "            idx_src = np.random.choice(len(src_words), batch_size)\n",
    "            idx_tgt = np.random.choice(len(tgt_words), batch_size)\n",
    "            src_batch = src_vecs[idx_src]\n",
    "            tgt_batch = tgt_vecs[idx_tgt]\n",
    "            mapped_src = src_batch @ W.T\n",
    "            \n",
    "            optimizer_D.zero_grad()\n",
    "            loss_D = nn.BCELoss()(discriminator(tgt_batch), torch.ones(batch_size, 1).to(device)) + \\\n",
    "                     nn.BCELoss()(discriminator(mapped_src.detach()), torch.zeros(batch_size, 1).to(device))\n",
    "            loss_D.backward()\n",
    "            optimizer_D.step()\n",
    "        \n",
    "        # Train generator (W)\n",
    "        idx_src = np.random.choice(len(src_words), batch_size)\n",
    "        src_batch = src_vecs[idx_src]\n",
    "        mapped_src = src_batch @ W.T\n",
    "        optimizer_W.zero_grad()\n",
    "        loss_W = nn.BCELoss()(discriminator(mapped_src), torch.ones(batch_size, 1).to(device))\n",
    "        loss_W.backward()\n",
    "        optimizer_W.step()\n",
    "        \n",
    "        # Orthogonalization\n",
    "        with torch.no_grad():\n",
    "            U, _, V = torch.svd(W)\n",
    "            W.copy_(U @ V.T)\n",
    "    \n",
    "    return W.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bd2a1e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_synthetic_dictionary(src_emb, tgt_emb, W, top_k=10000):\n",
    "    src_words = list(src_emb.keys())[:top_k]\n",
    "    tgt_words = list(tgt_emb.keys())\n",
    "    translations = {}\n",
    "    for src_word in src_words:\n",
    "        translations[src_word] = translate_word_csls(src_word, W, src_emb, tgt_emb, tgt_words, k=1)[0]\n",
    "    \n",
    "    # Filter mutual nearest neighbors\n",
    "    mutual_pairs = []\n",
    "    W_inv = np.linalg.inv(W)\n",
    "    for src, tgt in translations.items():\n",
    "        back_trans = translate_word_csls(tgt, W_inv, tgt_emb, src_emb, src_words, k=1)[0]\n",
    "        if back_trans == src:\n",
    "            mutual_pairs.append((src, tgt))\n",
    "    return mutual_pairs\n",
    "\n",
    "def refine_with_procrustes(pairs, src_emb, tgt_emb):\n",
    "    X = np.array([src_emb[src] for src, _ in pairs]).T\n",
    "    Y = np.array([tgt_emb[tgt] for _, tgt in pairs]).T\n",
    "    M = Y @ X.T\n",
    "    U, _, Vh = np.linalg.svd(M)\n",
    "    return U @ Vh\n",
    "\n",
    "def refine_mapping(src_emb, tgt_emb, W_init, iterations=5):\n",
    "    W = W_init\n",
    "    for _ in range(iterations):\n",
    "        pairs = extract_synthetic_dictionary(src_emb, tgt_emb, W)\n",
    "        if not pairs:\n",
    "            break\n",
    "        W = refine_with_procrustes(pairs, src_emb, tgt_emb)\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb668ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7611/864073911.py:20: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /croot/pytorch-select_1717607455294/work/torch/csrc/utils/tensor_new.cpp:274.)\n",
      "  src_vecs = torch.tensor([src_emb[w] for w in src_words], dtype=torch.float32).to(device)\n",
      "100%|██████████| 50/50 [00:09<00:00,  5.21it/s]\n"
     ]
    }
   ],
   "source": [
    "def evaluate_mapping(W, src_emb, tgt_emb, test_pairs, tgt_words, k_values=[1, 5]):\n",
    "    correct = {k: 0 for k in k_values}\n",
    "    for src, true_tgt in test_pairs:\n",
    "        pred_tgts = translate_word_csls(src, W, src_emb, tgt_emb, tgt_words, k=max(k_values))\n",
    "        for k in k_values:\n",
    "            if true_tgt in pred_tgts[:k]:\n",
    "                correct[k] += 1\n",
    "    precision = {k: correct[k] / len(test_pairs) for k in k_values}\n",
    "    return precision\n",
    "\n",
    "\n",
    "tgt_words = list(hi_embeddings.keys())\n",
    "\n",
    "# Unsupervised method\n",
    "W_adv = adversarial_training(en_embeddings, hi_embeddings)\n",
    "W_unsupervised = refine_mapping(en_embeddings, hi_embeddings, W_adv)\n",
    "precision_unsupervised = evaluate_mapping(W_unsupervised, en_embeddings, hi_embeddings, test_pairs, tgt_words)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Unsupervised Precision:\", precision_unsupervised)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098176f1",
   "metadata": {},
   "source": [
    "\n",
    "## Remarks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf16d6a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
